<!DOCTYPE html>
<html>
	<head>
		<title>Davidis's Kochbuch</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1"/>
        
        <link rel="stylesheet" href="bootstrap-3.3.7-dist/css/bootstrap.min.css">
        <link rel="stylesheet" href="css/myStyle.css">
        
		<script src="js/jquery.min.js"></script>
		<script src="bootstrap-3.3.7-dist/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script type="text/javascript" src="js/MathJax-2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
	</head>
	
	<body style="padding-top:50px"> <!-- padding because otherwise navbar is over the beginning of the content -->
        <div id="navbar"></div>

		<div class="container-fluid">
            <div class="row myHeaderForSideWithMyLeftNavBar">
                <div class="col-md-12">
                    <h1><a href="#">Evaluierung</a></h1>
                </div>
            </div>

            <div class="row">
                <div class="col-md-2">
                    <div class="list-group panel myLeftNavBar">
                        <a href="#Metrics" class="list-group-item">Recall &amp; Precision</a>
                        <a href="#CRFPrototyp" class="list-group-item">CRF-based Prototyp</a>
                        <a data-toggle="collapse" class="list-group-item" href="#collapse1">Dictionary- and rule-based Prototyp<span class="caret"></span></a>
                        <div id="collapse1" class="collapse">
                            <a href="#DictBasedV01" class="list-group-item">Version 0.1</a>
                            <a href="#DictBasedV02" class="list-group-item">Version 0.2</a>
                        </div>
                    </div>
                </div>
                
                <div class="col-md-10">
                    <p>Auf dieser Seite werden die <a href="AutomatischesAnreichernCueML.html">zuvor vorgestellten Prototypen</a> evaluiert. Beim CRF-based Prototyp wird erläutert, dass CRF für uns nicht umsetzbar ist. Die Dictionary- and rule-based Prototypen werten wir mit den Metriken Recall und Precision aus, weswegen wir diese als erstes einführen.
                    </p>
                    
                    <h2 id="Metrics" class="myAnchorWithFixedheader">Recall &amp; Precision</h2>
                    <p>Wir definieren Recall und Precision wie in den Formeln (1) und (2) nach <span cite="TextMining" class="beforePunctuation"></span>.
                    </p>
                    
                    <div class="row">
                        <div class="col-md-6">\begin{equation} Recall = \frac{\#(retrieved \cap relevant)}{\#relevant} \end{equation}</div>
                        <div class="col-md-6">\begin{equation} Precision = \frac{\#(retrieved \cap relevant)}{\#retrieved} \end{equation}</div>
                    </div>
                    
                    <p>Ein hoher Recall-Wert zeugt davon, dass viele relevante Informationen extrahiert werden. Ein hoher Precision-Wert zeugt davon, dass nur wenig falsche Informationen als relevant extrahiert werden. Beide Metriken sind nur zusammen aussagekräftig. Ein perfekter Recall-Wert von eins könnte ansonsten erreicht werden, in dem einfach alle Informationen als relevant extrahiert werden. Dies würde jedoch die Precision sehr wahrscheinlich ruinieren. Ein perfekter Wert von eins für die Precision könnte hingegen dadurch erreicht werden, dass keine Information als relevant extrahiert wird. Der Recall wäre dann allerdings null.
                    </p>
                    
                    <h2 id="CRFPrototyp" class="myAnchorWithFixedheader">CRF-based Prototyp</h2>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Antrainiert haben wir den CRF-based Prototypen mit <a href="#">9 Rezepten</a>. <a href="#" title="Link zu unserer digitalen Version des Rezeptes">Unser Test-Rezept</a> besteht aus 96 Wörtern. Von den dazugehörenden Labels sind 83 <i>O</i> für <i>others</i>.
                            </p>
                            
                            <p>Tabelle 1 zeigt die Stellen, wo sich die manuellen und die vom Prototypen extrahierten Labels unterscheiden. Die beiden Zutaten <i>Kartoffeln </i> und <i>Kartoffel-Klöße</i>, die nicht extrahiert wurden, sind auch nicht in den Trainings-Rezepten vorhanden. <i>Eine</i> und <i>einige</i> hingegen sind in den Trainingsdaten vorhanden. Sie sind mal als Mengenangabe gelabelt (z.B. in <i>eine geriebene gelbe Murzel</i>) und mal nicht (z.B. in <i>dies wird eine Weile gerührt</i>). Wenn dem CRF nur positive Gewichte erlaubt sind, orakelt er auch <i>Griesmehl</i> als <i>O</i>, obwohl Griesmehl in den Trainingsdaten nur einmal als <i>B-Ingredient</i> gelabelt vorkommt.
                            </p>
                            
                            <p>Tabelle 2 zeigt die Precision und Recall von dem CRF mit ausschließlich positiven, wie auch mit negativen Gewichten. Alle extrahierten Entities sind korrekt. Allerdings wurden 5 bzw. 6 der 13 Entitites des Rezeptes nicht als solche sondern als <i>O</i> gelabelt.
                            </p>
                        </div>

                        <div class="col-md-3">
                            <table border="1">
                                <caption>Tabelle 1: Unterschiede zwischen den manuellen und extrahierten Labels</caption>
                                <tr>
                                    <th>Wort im Satz</th>
                                    <th>Manuelle Labels</th>
                                    <th>Labels vom CRF</th>
                                </tr>
                                <tr>
                                    <td>eine</td>
                                    <td>B-Quantity</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>einige</td>
                                    <td>B-Quantity</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffeln</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffeln</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffel-Klöße</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                            </table>
                        </div>
                        
                        <div class="col-md-3">
                            <table border="1">
                                <caption>Tabelle 2:<br/> Precision and Recall</caption>
                                <tr>
                                    <th></th>
                                    <th>Precision</th>
                                    <th>Recall</th>
                                </tr>
                                <tr>
                                    <th>Prototyp mit negativen Gewichten</th>
                                    <td>100,00%</td>
                                    <td>61,54%</td>
                                </tr>
                                <tr>
                                    <th>Prototyp ohne negativen Gewichten</th>
                                    <td>100,00%</td>
                                    <td>53,85%</td>
                                </tr>
                            </table>
                        </div>
                    </div>
                    
                    
                    <p>Das Antrainieren mit 9 Rezepten, sowie das Auswerten anhand nur einem Rezept ist natürlich kein Maßstab für eine echte Evaluierung. Allerdings sind uns bereits an diesem simplen Prototypen viele Stolpersteine aufgefallen.
                    </p>
                
                    <div class="row">
                        <div class="col-md-9">
                            <p>
                                <figure myModal="images/AutomatischesAnreichrenCueML/cueMLvonAusschnittCRFTrainingsdaten.png">
                                    <figcaption>Abb. 1: Zu Abb. 2 entsprechender mit cueML ausgezeichneter Text</figcaption>
                                    <img src="images/AutomatischesAnreichrenCueML/cueMLvonAusschnittCRFTrainingsdaten.png"/>
                                </figure>
                            </p>
                            
                            <p>Abb. 1 zeigt den, zu den gelabelten Trainingsdaten aus Abb. 2, entsprechenden mit cueML ausgezeichneten Text. Bereits in diesem kleinen Ausschnitt der Trainingsdaten lassen sich viele Stolpersteine finden:
                            </p>
                            
                            <ul>
                                <li><i>"ein bis zwei Eßlöffel"</i> sind in cueML eindeutig mit den Attributen <i>atLeast="1"</i> und <i>atMost="2"</i> abzubilden. <b>Eine eindeutige Abbildung auf Labels ist jedoch schwieriger</b>. Um den reinen Text mittels den Labels aus Abb. 2 mit cueML auszuzeichnen, müssten diese noch in die entsprechenden Attribute übersetzt werden. Alternativ hätte man auch folgende Labels überlegen können; <i>ein (B-atLeast) bis (O) zwei (B-atMost)</i> oder <i>ein (B-atLeast) bis (B-IndicatorForAtLeastAtMost) zwei (B-atMost).</i>
                                </li>
                                <li>Ungeachtet davon, welche Labels beispielsweise für <i>ein bis zwei</i> verwendet wurden, geht aus ihnen noch nicht hervor, dass sich diese Mengenangabe auf das <i>Mehl</i> bezieht. Wir stehen damit vor dem allgemeinen Problem, <b>dass mittels CRF zwar Entities extrahiert werden können, aber nicht Beziehungen zwischen diesen</b>. <span cite="CRFNYT"></span> hatte dieses Problem nicht. Wenn ein CRF-Orakel auf jede Zeile einer bestehenden Zutatenliste angewendet wird, ist klar, dass sich die extrahierte Mengenangabe auf die <b>eine</b> Zutat bezieht. Bei seinem Beispiel <i>"4 tablespoons melted nonhydrogenated margarine, melted coconut oil or canola oil"</i> stellt er selber fest, dass sein CRF dafür nicht gerüstet ist (<i>"This ingredient phrase contains multiple ingredient names, which is a situation that is not accounted for in our database schema. We need to rethink the way we label ingredient parts to account for examples like this."</i> <span cite="CRFNYT" class="beforePunctuation"></span>).
                                </li>
                                <li>Es ist schwierig aus dem cueML-Attribut <i>quantity="1"</i> abzuleiten, dass zu dem vorherigen Wort <i>eine</i> das Label <i>B-Quantity</i> gehört. Daher muss <b>das Labeln der Trainingsdaten manuell erledigt werden, obwohl es bereits mit cueML ausgezeichnete Rezepte gibt</b>. Aus den 10 verwendeten Rezepten ergeben sich mehr als 1.500 Zeilen, die gelabelt werden müssen. Wie <span cite="CRFNYT"></span> den CRF mit mehr als 130.000 Trainingsdaten anzutrainieren, ist somit ein nicht zu stemmender Aufwand. Dies gilt insbesondere, da nach dem ersten Punkt die zu verwendeten Labels nicht eindeutig sind. Dementsprechend müssen unterschiedliche Labels evaluiert werden und somit die Trainingsdaten mehrmals mit unterschiedlichen Labels erstellt werden.
                                </li>
                                <li>Im Gegensatz zu <span cite="CRFNYT"></span> labeln wir ganze Sätze und nicht nur eine Zutatenliste. <b>Die meisten unserer Labels sind daher <i>O</i> für Other</b>. Dies führt dazu, dass das CRF-Orakel Wörter wie <i>eine</i> bevorzugt als <i>O</i> orakelt anstatt als <i>B-Quantity</i>.
                                </li>
                                <li>Daran das <i>Kartoffeln</i> nicht erkannt wird, wird auch deutlich, <b>dass der CRF nur Wörter als Zutaten erkennen kann, die auch in den Trainings&shy;daten waren</b>. Wäre in den Trainingsdaten <i>eine Kartoffel</i> vorhanden, so könnte der CRF <i>Kartoffeln</i> immer noch nicht erkennen. Daher ist als custom feature das Lemma des Wortes nötig. Wenn wir nun allerdings davon ausgehen, dass wir alle Zutaten (in den Trainingsdaten) kennen und zusätzlich die Lemmatas der Wörter haben, können wir die Zutaten-Entities auch über einen Wörterbuch-Check extrahieren und brauchen den CRF dafür nicht.
                                </li>
                            </ul>
                        </div>
                        
                        <div class="col-md-3">
                            <figure myModal="images/AutomatischesAnreichrenCueML/AusschnittCRFTrainingsdaten.png">
                                <figcaption>Abb. 2: Ausschnitt unserer CRF Trainingsdaten</figcaption>
                                <img src="images/AutomatischesAnreichrenCueML/AusschnittCRFTrainingsdaten.png"/>
                            </figure>
                        </div>
                    </div>
                    
                    <p>Des Weiteren ist uns unklar, <b>wie die Unterscheidung zwischen Zutaten, optionalen Zutaten, alternativen Zutaten und nicht zu verwendenten Zutaten zu modelieren ist</b>. Für diese sind auf jeden Fall eigene Labels nötig. Da die Wörter/Zutaten zu den unterschiedlichen Labels jeweils die selben sind, braucht das CRF zur Unterscheidung wahrscheinlich zusätzlich weitere Labels und Features. Diese zu entwickeln und zu evaluieren ist jedoch wieder aufwendig.
                    </p>
                    
                    <p>Wir wollen damit nicht ausschließen, dass es möglich ist, die Zutaten mittels CRF zu extrahieren. Allerdings ist dieser Algorithmus bei den schlecht strukturierten Zubereitungs-Anweisungen von Frau Davides's mit vielen Stolpersteinen verbunden. Eine sinnvolle Umsetzung ist daher aus den oben genannten Gründen nicht im Rahmen dieser Arbeit zu erledigen. Stattdessen haben wir den <i>Dictionary- and rule-based</i>-Ansatz weiterverfolgt.
                    </p>
                    
                    
                    
                    <h2 class="myAnchorWithFixedheader">Dictionary- and rule-based Prototyp</h2>
                    <p>Unsere Dictionary- and rule-based Prototypen testen wir anhand von Recall and Precision. Wir haben dazu die <a href="DavidisesKochbuch/Rezepte%20mit%20cueML.xml">die ersten 50 Suppen von Frau Davidis's Kochbuch</a> manuell ausgezeichnet und verwenden dies als Golden Standard.
                    </p>
                    
                    <h3 id="DictBasedV01" class="myAnchorWithFixedheader">Version 0.1</h3>
                    
                    <p>Das Ziel dieses Prototypens ist es zu testen, ob unsere gesuchten Entities (Zutaten, Mengenangaben und Einheiten) extrahiert werden können. Da bei den Mengenangaben und Einheiten meine Intuition nicht Alarm schlägt, reichen mir ein paar Beispiel Daten, um zu verifizieren, dass das funktioniert. Dies scheint der Fall zu sein. Daher wird eine genauere Evaluation der Mengenangaben und Einheiten auf einen ausgereifteren Prototypen verschoben.
                    </p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Abb. 3 zeigt, wie wir den <b>Recall</b> ausgewertet haben. Idealerweise ist in den extrahierten Zutaten jede Zutat wiederzufinden, die in unserem Golden Standard auch manuell ausgezeichnet wurden. Ist ein Wort im Golden Standard Rezept als Zutat ausgezeichnet, überprüfen wir einfach, ob unser Prototyp an der gleichen Stelle eine Zutat extrahiert hat.
                            </p>
                        </div>
                        
                        <div class="col-md-6">
                            <figure myModal="images/Evaluierung/recall.png">
                                <figcaption>Abb. 3: Recall</figcaption>
                                <img src="images/Evaluierung/recall.png"/>
                            </figure>
                        </div>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Die Auswertung der <b>Precision</b> ist komplexer, wie in Abb. 4 zu sehen ist. Wenn eine Zutat in unserem Golden Standard Rezept mehrfach erwähnt wird, ist es valide, dass sie dort nur einmal als solche ausgezeichnet ist. Das unser Prototyp alle Vorkommen auszeichnet, ist kein Fehler. Die Relevanz kann aber somit nicht mehr ausschließelich über die Wortposition verifiziert werden. Wenn die Verifizierung über die Wortposition fehlschlägt, versuchen wir es daher anschließend noch über die möglichen <i>ref</i>-Elemente. Beim Nachschlagen in unserem Zutaten-Wörterbuch speichern wir uns die möglichen <i>xml:ids</i>. Ist im Golden Standard Rezept einer der möglichen extrahierten xml:ids ausgezeichnet, wird die extrahierte Zutat ebenfalls als relevant evaluiert.
                            </p>
                        </div>
                        
                        <div class="col-md-6">
                            <p>
                                <figure myModal="images/Evaluierung/precision.png">
                                    <figcaption>Abb. 4: Precision</figcaption>
                                    <img src="images/Evaluierung/precision.png"/>
                                </figure>
                            </p>
                        </div>
                    </div>
                    
                    <p>Die Extraktion und Evaluation der ersten 50 Suppen dauert <b>knapp 2:30 Min</b>. Der <b>Recall</b> liegt bei <b>90%</b> (457 der 506 Zutaten wurden extrahiert) und die <b>Precision</b> ist <b>88%</b> (511 von 583 extrahierten Zutaten sind relevant). Diese Ergebnisse zeugen davon, dass die dictionary-based Extraktion funktioniert.
                    </p>
                    
                    
                    <h3 id="DictBasedV02" class="myAnchorWithFixedheader">Version 0.2</h3>
                    
                    <p>Folgend testen wir nicht mehr nur die Machbarkeitsanalyse, sondern wollen eine echte Evaluierung. Daher machen wir uns zunächst genauere Gedanken, wie wir evaluieren, ob eine extrahierte Entitiy bezüglich einem manuell getagtem Rezept relevant und korrekt ist. Wie in Abb. 4 zu sehen ist, brauchen wir zwei unterschiedliche Vergleich-Ansätze, die wir <b>Exact match</b> und <b>Rough match</b> nennen wollen:
                    </p>
                    <div class="myIndent">
                        <p>Ein <b>Exact match</b> kann nur vorliegen, wenn an gleicher Position im Rezept eine Zutat extrahiert wurde. Des Weiteren evaluieren wir alle weiteren extrahierten Entities anhand der Zutaten, da nur diese ausgezeichnet werden. Wenn beispielsweise korrekterweise <i>2 EL</i> extrahiert wurde, aber es keiner Zutat zugeordnet werden kann, ist die extrahierte Information wertlos.  In der <i>Option 2</i> muss für einen exact match zusätzlich zu einer Zutat die Mengenangabe (das <i>quantitiy</i>-Attribut) und Einheit (<i>unit</i>-Attribut) korrekt extrahiret sein. <i>Option 3</i> überprüft zusätzlich die Attribute <i>optional</i> und <i>altGrp</i>. Die zu überprüfenden Attribute werden im Programm in <i>main.py</i> in der <i>attris</i>-variabel gesetzt.
                        </p>
                        
                        <p>Wenn bei der Precision kein Exact match vorliegt, folgt daraus nicht sofort, dass die Zutat unrelevant ist, wie in Abb. 4 zu sehen ist. Wenn die extrahierte Zutat kein Exact match ist, aber ebenfalls an einer anderen Stelle im Rezept extrahiert wurde und sie an der anderen Stelle ein Exact Match ist, liegt ein <b>Rough match</b> vor. Da diese Zutat weder unrelevant ist, noch zu den relevanten Zutaten gehört, ignorieren wir sie für die Evaluation.
                        </p>
                    </div>
                    
                        
                    <p>Tabelle 3 zeigt die Ergebnisse mit unserer finalen Version 0.2.
                    </p>
                    
                    
                    <table border="1" style="margin-bottom: 1em;">
                        <caption>Tabelle 3</caption>
                        <tr>
                            <th></th>
                            <th>Recall</th>
                            <th>Precision</th>
                        </tr>
                        <tr>
                            <th>Option 1 (nur Zutaten): </th>
                            <td>98,62% (499 von 506)</td>
                            <td>98,23% (500 von 509)</td>
                        </tr>
                        <tr>
                            <th>Option 2 (mit Mengenangaben und Einheiten): </th>
                            <td>84,78% (429 von 506)</td>
                            <td>83,66% (430 von 514)</td><!-- weil roughMatch nicht mehr funktioniert mehr als 509 -->
                        </tr>
                        <tr>
                            <th>Option 3 (mit optionalen und alternativen Zutaten): </th>
                            <td>71,34% (361 von 506</td>
                            <td>72,84% (362 von 497)</td><!-- 2. erwähnung einer Zutat -> optional vorher exakt, nun aber durch roughMatch -->
                        </tr>
                    </table>
                    
                    <p>Der Recall und die Precision der Option 1 von fast 100% zeigen, dass nahezu alle Zutaten nach den Verbesserungen in der Version 0.2 extrahiert werden. Die verbliebenen Fehler sind <a href="verbliebeneFehler.html">hier</a> aufgelistet.
                    </p>
                    
                    <p>Option 2 zeigt, dass wir zu mehr als 8 von 10 Zutaten die Mengenangabe und Einheit richtig zuordnen. Wir haben dabei zwei systematische Fehler festgestellt. Zu einem können wir textuelle Mengenspannen der Form <i>"auf jede Person ¾ Pfund, bei einer großen Gesellschaft ½ Pfund"</i> nicht extrahieren. Stattdessen wird nur ½ Pfund extrahiert. Zum anderen sind unsere vagen Mengenwörter wie <i>ein</i> oder <i>eine</i> Problematisch. Wenn wir diese bei der Extraktion verwendeten wird zum Beispiel in <i>"gibt man die Brühe [...] ein Sieb, schwitzt [...] Mehl in Butter [...]"</i> fälschlicherweise die Mengenangabe <i>ein/1</i> für Mehl extrahiert. Wenn wir solche Mengenwörter nicht berücksichtigen wird jedoch die Mengenanagabe <i>1</i> beispielsweise bei <i>"gibt man eine fein geschnittene Zwiebel hinzu"</i> nicht erkannt. In beiden Varianten gehen gut 20 Fehler auf die Verwendung bzw. nicht Verwendung solcher Mengenwörter zurück. Um die Mengenwörtern von den unbestimmten Artikeln besser differenzieren zu können, werden offensichtlich weitere Regeln benötigt.
                    </p>
                    
                    <p>Unsere zwei beispielhaften Regeln für optionale und alternative Zutaten triggern so gut wie gar nicht. Dementsprechend geht der Recall und die Precision in der Option 3 stark nach unten. Das Differnezieren dieser Entities durch Regeln muss definitiv noch weiter erforscht werden.
                    </p>
                    
                    <p>Dass die Anzahl unser extrahierten Entities mit den Optionen variert, liegt an unsrerem Rough match. In Option 1 wird z.B. im Rezept B-1 das erste Vorkommen von Fleisch als Exact match identifiziert. Alle weiteren Vorkommen werden daher durch einen Rough match vom Ergebnis ignoriert. In Option 2 wird jedoch beim ersten Vorkommen von Fleisch die Mengenangabe als falsch evaluiert. Daher schlägt anschließend auch der Rough match fehl und alle weiteren Vorkommen von Fleisch werden als Fehler identifiziert. In der Option 3 kommt es vor, dass Zutaten nun nicht mehr ein Exact match sind, da ihnen das <i>optional</i>-Attribut fehlt. Statt dessen werden sie jedoch durch einen Rough match gefangen. Daher tauchen in der Option 3 insgesammt weniger Entities in der Precision auf.
                    </p>
                    
                    <p>Insgesamt werten wir den dictionary- and rule-based Prototypen als viel versprechend, auch wenn in zukünftigen Arbeiten mehr als die einfachen Beispiel-Regeln entwickeln werden müssen.
                    </p>
                    
                </div>
                
            </div>
        </div>
		
        
        <script>activeTab = "DigitaleEdition"</script>
        <script src="js/navbar.js"></script>
        <script src="js/figure2modal.js"></script>
        <script src="js/references.js"></script>
	</body>
</html> 