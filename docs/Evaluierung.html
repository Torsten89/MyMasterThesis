<!DOCTYPE html>
<html>
	<head>
		<title>Davidis's Kochbuch</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1"/>
        
        <link rel="stylesheet" href="bootstrap-3.3.7-dist/css/bootstrap.min.css">
        <link rel="stylesheet" href="css/myStyle.css">
        
		<script src="js/jquery.min.js"></script>
		<script src="bootstrap-3.3.7-dist/js/bootstrap.min.js"></script>
        <script type="text/x-mathjax-config">
            MathJax.Hub.Config({
              TeX: { equationNumbers: { autoNumber: "AMS" } }
            });
        </script>
        <script type="text/javascript" src="js/MathJax-2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
	</head>
	
	<body style="padding-top:50px"> <!-- padding because otherwise navbar is over the beginning of the content -->
        <div id="navbar"></div>

		<div class="container-fluid">
            <div class="row myHeaderForSideWithMyLeftNavBar">
                <div class="col-md-12">
                    <h1><a href="#">Evaluierung</a></h1>
                </div>
            </div>

            <div class="row">
                <div class="col-md-2">
                    <div class="list-group panel myLeftNavBar">
                        <a href="#Metrics" class="list-group-item">Recall &amp; Precision</a>
                        <a href="#CRFPrototyp" class="list-group-item">CRF-based Prototyp</a>
                        <a data-toggle="collapse" class="list-group-item" href="#collapse1">Dictionary- and rule-based Prototyp<span class="caret"></span></a>
                        <div id="collapse1" class="collapse">
                            <a href="#DictBasedV01" class="list-group-item">Version 0.1</a>
                            <a href="#DictBasedV02" class="list-group-item">Version 0.2</a>
                        </div>
                    </div>
                </div>
                
                <div class="col-md-10">
                    <p>Auf dieser Seite werden die <a href="AutomatischesAnreichernCueML.html">zuvor vorgestellten Prototypen</a> evaluiert. Beim CRF-based Prototyp wird erläutert, dass CRF für uns nicht umsetzbar ist. Die Dictionary- and rule-based Prototypen werten wir mit den Metriken Recall und Precision aus, weswegen wir diese als erstes einführen.
                    </p>
                    
                    <h2 id="Metrics" class="myAnchorWithFixedheader">Recall &amp; Precision</h2>
                    <p>Wir definieren Recall und Precision wie in den Formeln (1) und (2) nach <span cite="TextMining" class="beforePunctuation"></span>.
                    </p>
                    
                    <div class="row">
                        <div class="col-md-6">\begin{equation} Recall = \frac{\#(retrieved \cap relevant)}{\#relevant} \end{equation}</div>
                        <div class="col-md-6">\begin{equation} Precision = \frac{\#(retrieved \cap relevant)}{\#retrieved} \end{equation}</div>
                    </div>
                    
                    <p>Ein hoher Recall-Wert zeugt davon, dass viele relevante Informationen extrahiert werden. Ein hoher Precision-Wert zeugt davon, dass nur wenig falsche Informationen als relevant extrahiert werden. Beide Metriken sind nur zusammen aussagekräftig. Ein perfekter Recall-Wert von eins könnte ansonsten erreicht werden, in dem einfach alle Informationen als relevant extrahiert werden. Dies würde jedoch die Precision sehr wahrscheinlich ruinieren. Ein perfekter Wert von eins für die Precision könnte hingegen dadurch erreicht werden, dass keine Information als relevant extrahiert wird. Der Recall wäre dann allerdings null.
                    </p>
                    
                    <h2 id="CRFPrototyp" class="myAnchorWithFixedheader">CRF-based Prototyp</h2>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Antrainiert haben wir den CRF-based Prototypen mit <a href="#">9 Rezepten</a>. <a href="#" title="Link zu unserer digitalen Version des Rezeptes">Unser Test-Rezept</a> besteht aus 96 Wörtern. Von den dazugehörenden Labels sind 83 <i>O</i> für <i>others</i>.
                            </p>
                            
                            <p>Tabelle 1 zeigt die Stellen, wo sich die manuellen und die vom Prototypen extrahierten Labels unterscheiden. Die beiden Zutaten <i>Kartoffeln </i> und <i>Kartoffel-Klöße</i>, die nicht extrahiert wurden, sind auch nicht in den Trainings-Rezepten vorhanden. <i>Eine</i> und <i>einige</i> hingegen sind in den Trainingsdaten vorhanden. Sie sind mal als Mengenangabe gelabelt (z.B. in <i>eine geriebene gelbe Murzel</i>) und mal nicht (z.B. in <i>dies wird eine Weile gerührt</i>). Wenn dem CRF nur positive Gewichte erlaubt sind, orakelt er auch <i>Griesmehl</i> als <i>O</i>, obwohl Griesmehl in den Trainingsdaten nur einmal als <i>B-Ingredient</i> gelabelt vorkommt.
                            </p>
                            
                            <p>Tabelle 2 zeigt die Precision und Recall von dem CRF mit ausschließlich positiven, wie auch mit negativen Gewichten. Alle extrahierten Entities sind korrekt. Allerdings wurden 5 bzw. 6 der 13 Entitites des Rezeptes nicht als solche sondern als <i>O</i> gelabelt.
                            </p>
                        </div>

                        <div class="col-md-3">
                            <table border="1">
                                <caption>Tabelle 1: Unterschiede zwischen den manuellen und extrahierten Labels</caption>
                                <tr>
                                    <th>Wort im Satz</th>
                                    <th>Manuelle Labels</th>
                                    <th>Labels vom CRF</th>
                                </tr>
                                <tr>
                                    <td>eine</td>
                                    <td>B-Quantity</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>einige</td>
                                    <td>B-Quantity</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffeln</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffeln</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                                <tr>
                                    <td>Kartoffel-Klöße</td>
                                    <td>B-Ingredient</td>
                                    <td>O</td>
                                </tr>
                            </table>
                        </div>
                        
                        <div class="col-md-3">
                            <table border="1">
                                <caption>Tabelle 2:<br/> Precision and Recall</caption>
                                <tr>
                                    <th></th>
                                    <th>Precision</th>
                                    <th>Recall</th>
                                </tr>
                                <tr>
                                    <th>Prototyp mit negativen Gewichten</th>
                                    <td>100,00%</td>
                                    <td>61,54%</td>
                                </tr>
                                <tr>
                                    <th>Prototyp ohne negativen Gewichten</th>
                                    <td>100,00%</td>
                                    <td>53,85%</td>
                                </tr>
                            </table>
                        </div>
                    </div>
                    
                    
                    <p>Das Antrainieren mit 9 Rezepten, sowie das Auswerten anhand nur einem Rezept ist natürlich kein Maßstab für eine echte Evaluierung. Allerdings sind uns bereits an diesem simplen Prototypen viele Stolpersteine aufgefallen.
                    </p>
                
                    <div class="row">
                        <div class="col-md-9">
                            <p>
                                <figure myModal="images/AutomatischesAnreichrenCueML/cueMLvonAusschnittCRFTrainingsdaten.png">
                                    <figcaption>Abb. 1: Zu Abb. 2 entsprechender mit cueML ausgezeichneter Text</figcaption>
                                    <img src="images/AutomatischesAnreichrenCueML/cueMLvonAusschnittCRFTrainingsdaten.png"/>
                                </figure>
                            </p>
                            
                            <p>Abb. 1 zeigt den, zu den gelabelten Trainingsdaten aus Abb. 2, entsprechenden mit cueML ausgezeichneten Text. Bereits in diesem kleinen Ausschnitt der Trainingsdaten lassen sich viele Stolpersteine finden:
                            </p>
                            
                            <ul>
                                <li><i>"ein bis zwei Eßlöffel"</i> sind in cueML eindeutig mit den Attributen <i>atLeast="1"</i> und <i>atMost="2"</i> abzubilden. <b>Eine eindeutige Abbildung auf Labels ist jedoch schwieriger</b>. Um den reinen Text mittels den Labels aus Abb. 2 mit cueML auszuzeichnen, müssten diese noch in die entsprechenden Attribute übersetzt werden. Alternativ hätte man auch folgende Labels überlegen können; <i>ein (B-atLeast) bis (O) zwei (B-atMost)</i> oder <i>ein (B-atLeast) bis (B-IndicatorForAtLeastAtMost) zwei (B-atMost).</i>
                                </li>
                                <li>Ungeachtet davon, welche Labels beispielsweise für <i>ein bis zwei</i> verwendet wurden, geht aus ihnen noch nicht hervor, dass sich diese Mengenangabe auf das <i>Mehl</i> bezieht. Wir stehen damit vor dem allgemeinen Problem, <b>dass mittels CRF zwar Entities extrahiert werden können, aber nicht Beziehungen zwischen diesen</b>. <span cite="CRFNYT"></span> hatte dieses Problem nicht. Wenn ein CRF-Orakel auf jede Zeile einer bestehenden Zutatenliste angewendet wird, ist klar, dass sich die extrahierte Mengenangabe auf die <b>eine</b> Zutat bezieht. Bei seinem Beispiel <i>"4 tablespoons melted nonhydrogenated margarine, melted coconut oil or canola oil"</i> stellt er selber fest, dass sein CRF dafür nicht gerüstet ist (<i>"This ingredient phrase contains multiple ingredient names, which is a situation that is not accounted for in our database schema. We need to rethink the way we label ingredient parts to account for examples like this."</i> <span cite="CRFNYT" class="beforePunctuation"></span>).
                                </li>
                                <li>Es ist schwierig aus dem cueML-Attribut <i>quantity="1"</i> abzuleiten, dass zu dem vorherigen Wort <i>eine</i> das Label <i>B-Quantity</i> gehört. Daher muss <b>das Labeln der Trainingsdaten manuell erledigt werden, obwohl es bereits mit cueML ausgezeichnete Rezepte gibt</b>. Aus den 10 verwendeten Rezepten ergeben sich mehr als 1.500 Zeilen, die gelabelt werden müssen. Wie <span cite="CRFNYT"></span> den CRF mit mehr als 130.000 Trainingsdaten anzutrainieren, ist somit ein nicht zu stemmender Aufwand. Dies gilt insbesondere, da nach dem ersten Punkt die zu verwendeten Labels nicht eindeutig sind. Dementsprechend müssen unterschiedliche Labels evaluiert werden und somit die Trainingsdaten mehrmals mit unterschiedlichen Labels erstellt werden.
                                </li>
                                <li>Im Gegensatz zu <span cite="CRFNYT"></span> labeln wir ganze Sätze und nicht nur eine Zutatenliste. <b>Die meisten unserer Labels sind daher <i>O</i> für Other</b>. Dies führt dazu, dass das CRF-Orakel Wörter wie <i>eine</i> bevorzugt als <i>O</i> orakelt anstatt als <i>B-Quantity</i>.
                                </li>
                                <li>Daran das <i>Kartoffeln</i> nicht erkannt wird, wird auch deutlich, <b>dass der CRF nur Wörter als Zutaten erkennen kann, die auch in den Trainings&shy;daten waren</b>. Wäre in den Trainingsdaten <i>eine Kartoffel</i> vorhanden, so könnte der CRF <i>Kartoffeln</i> immer noch nicht erkennen. Daher ist als custom feature das Lemma des Wortes nötig. Wenn wir nun allerdings davon ausgehen, dass wir alle Zutaten (in den Trainingsdaten) kennen und zusätzlich die Lemmatas der Wörter haben, können wir die Zutaten-Entities auch über einen Wörterbuch-Check extrahieren und brauchen den CRF dafür nicht.
                                </li>
                            </ul>
                        </div>
                        
                        <div class="col-md-3">
                            <figure myModal="images/AutomatischesAnreichrenCueML/AusschnittCRFTrainingsdaten.png">
                                <figcaption>Abb. 2: Ausschnitt unserer CRF Trainingsdaten</figcaption>
                                <img src="images/AutomatischesAnreichrenCueML/AusschnittCRFTrainingsdaten.png"/>
                            </figure>
                        </div>
                    </div>
                    
                    <p>Des Weiteren ist uns unklar, <b>wie die Unterscheidung zwischen Zutaten, optionalen Zutaten, alternativen Zutaten und nicht zu verwendenten Zutaten zu modelieren ist</b>. Für diese sind auf jeden Fall eigene Labels nötig. Da die Wörter/Zutaten zu den unterschiedlichen Labels jeweils die selben sind, braucht das CRF zur Unterscheidung wahrscheinlich zusätzlich weitere Labels und Features. Diese zu entwickeln und zu evaluieren ist jedoch wieder aufwendig.
                    </p>
                    
                    <p>Wir wollen damit nicht ausschließen, dass es möglich ist, die Zutaten mittels CRF zu extrahieren. Allerdings ist dieser Algorithmus bei den schlecht strukturierten Zubereitungs-Anweisungen von Frau Davides's mit vielen Stolpersteinen verbunden. Eine sinnvolle Umsetzung ist daher aus den oben genannten Gründen nicht im Rahmen dieser Arbeit zu erledigen. Stattdessen haben wir den <i>Dictionary- and rule-based</i>-Ansatz weiterverfolgt.
                    </p>
                    
                    
                    
                    <h2 class="myAnchorWithFixedheader">Dictionary- and rule-based Prototyp</h2>
                    <p>Unsere Dictionary- and rule-based Prototypen testen wir anhand von Recall and Precision. Wir haben dazu die <a href="DavidisesKochbuch/Rezepte%20mit%20cueML.xml">die ersten 50 Suppen von Frau Davidis's Kochbuch</a> manuell ausgezeichnet und verwenden dies als Golden Standard.
                    </p>
                    
                    <h3 id="DictBasedV01" class="myAnchorWithFixedheader">Version 0.1</h3>
                    
                    <p>Das Ziel dieses Prototypens ist es zu testen, ob unsere gesuchten Entities (Zutaten, Mengenangaben und Einheiten) extrahiert werden können. Da bei den Mengenangaben und Einheiten meine Intuition nicht Alarm schlägt, reichen mir ein paar Beispiel Daten, um zu verifizieren, dass das funktioniert. Dies scheint der Fall zu sein. Daher wird eine genauere Evaluation der Mengenangaben und Einheiten auf einen ausgereifteren Prototypen verschoben.
                    </p>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Abb. 3 zeigt, wie wir den <b>Recall</b> ausgewertet haben. Idealerweise ist in den extrahierten Zutaten jede Zutat wiederzufinden, die in unserem Golden Standard auch manuell ausgezeichnet wurden. Ist ein Wort im Golden Standard Rezept als Zutat ausgezeichnet, überprüfen wir einfach, ob unser Prototyp an der gleichen Stelle eine Zutat extrahiert hat.
                            </p>
                        </div>
                        
                        <div class="col-md-6">
                            <figure myModal="images/Evaluierung/recall.png">
                                <figcaption>Abb. 3: Recall</figcaption>
                                <img src="images/Evaluierung/recall.png"/>
                            </figure>
                        </div>
                    </div>
                    
                    <div class="row">
                        <div class="col-md-6">
                            <p>Die Auswertung der <b>Precision</b> ist komplexer, wie in Abb. 4 zu sehen ist. Wenn eine Zutat in unserem Golden Standard Rezept mehrfach erwähnt wird, ist es valide, dass sie dort nur einmal als solche ausgezeichnet ist. Das unser Prototyp alle Vorkommen auszeichnet, ist kein Fehler. Die Relevanz kann aber somit nicht mehr ausschließelich über die Wortposition verifiziert werden. Wenn die Verifizierung über die Wortposition fehlschlägt, versuchen wir es daher anschließend noch über die möglichen <i>ref</i>-Elemente. Beim Nachschlagen in unserem Zutaten-Wörterbuch speichern wir uns die möglichen <i>xml:ids</i>. Ist im Golden Standard Rezept einer der möglichen extrahierten xml:ids ausgezeichnet, wird die extrahierte Zutat ebenfalls als relevant evaluiert.
                            </p>
                        </div>
                        
                        <div class="col-md-6">
                            <p>
                                <figure myModal="images/Evaluierung/precision.png">
                                    <figcaption>Abb. 4: Precision</figcaption>
                                    <img src="images/Evaluierung/precision.png"/>
                                </figure>
                            </p>
                        </div>
                    </div>
                    
                    <p>Die Extraktion und Evaluation der ersten 50 Suppen dauert <b>knapp 2:30 Min</b>. Der <b>Recall</b> liegt bei <b>90%</b> (457 der 506 Zutaten wurden extrahiert) und die <b>Precision</b> ist <b>88%</b> (511 von 583 extrahierten Zutaten sind relevant). Diese Ergebnisse zeugen davon, dass die dictionary-based Extraktion funktioniert.
                    </p>
                    
                    
                    <h3 id="DictBasedV02" class="myAnchorWithFixedheader">Version 0.2</h3>
                    <p><a href="verbliebeneFehler.html">verbliebene Fehler Zutaten extrahieren</a>
                    </p>
                    <table border="1">
                        <caption>Tabelle 3</caption>
                        <tr>
                            <th></th>
                            <th>Recall</th>
                            <th>Precision</th>
                        </tr>
                        <tr>
                            <th>Option 1 (nur Zutaten): </th>
                            <td>98,62% (499 von 506)</td>
                            <td>98,23% (500 von 509)</td>
                        </tr>
                        <tr>
                            <th>Option 2 (mit Mengenangaben und Einheiten): </th>
                            <td>84,78% (429 von 506)</td>
                            <td>83,66% (430 von 514)</td><!-- weil roughMatch nicht mehr funktioniert mehr als 509 -->
                        </tr>
                        <tr>
                            <th>Option 3 (mit optionalen und alternativen Zutaten): </th>
                            <td>71,34% (361 von 506</td>
                            <td>72,84% (362 von 497)</td><!-- 2. erwähnung einer Zutat -> optional vorher exakt, nun aber durch roughMatch -->
                        </tr>
                    </table>
                    
                </div>
                
            </div>
        </div>
		
        
        <script>activeTab = "DigitalEdition"</script>
        <script src="js/navbar.js"></script>
        <script src="js/figure2modal.js"></script>
        <script src="js/references.js"></script>
	</body>
</html> 